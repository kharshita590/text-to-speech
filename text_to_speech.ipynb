{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qjwVz84SOWXp"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "!pip install tts\n",
        "!pip install fastapi uvicorn\n",
        "!pip install pyngrok\n",
        "!pip install python-multipart\n",
        "!pip install nest_asyncio\n",
        "!pip uninstall numpy -y\n",
        "!pip install --upgrade pip setuptools wheel\n",
        "!pip install TTS\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install numpy==1.23.5\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 724
        },
        "id": "IsswULpGWKjD",
        "outputId": "0020e5e5-a4f1-4a57-895e-f9f906533dbd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting numpy==1.23.5\n",
            "  Using cached numpy-1.23.5-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (2.3 kB)\n",
            "Using cached numpy-1.23.5-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (17.1 MB)\n",
            "Installing collected packages: numpy\n",
            "  Attempting uninstall: numpy\n",
            "    Found existing installation: numpy 1.26.4\n",
            "    Uninstalling numpy-1.26.4:\n",
            "      Successfully uninstalled numpy-1.26.4\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "tts 0.22.0 requires numpy>=1.24.3; python_version > \"3.10\", but you have numpy 1.23.5 which is incompatible.\n",
            "google-colab 1.0.0 requires pandas==2.2.2, but you have pandas 1.5.3 which is incompatible.\n",
            "tensorflow 2.18.0 requires numpy<2.1.0,>=1.26.0, but you have numpy 1.23.5 which is incompatible.\n",
            "dask-cudf-cu12 25.2.2 requires pandas<2.2.4dev0,>=2.0, but you have pandas 1.5.3 which is incompatible.\n",
            "cudf-cu12 25.2.1 requires pandas<2.2.4dev0,>=2.0, but you have pandas 1.5.3 which is incompatible.\n",
            "treescope 0.1.9 requires numpy>=1.25.2, but you have numpy 1.23.5 which is incompatible.\n",
            "pymc 5.21.1 requires numpy>=1.25.0, but you have numpy 1.23.5 which is incompatible.\n",
            "blosc2 3.2.1 requires numpy>=1.26, but you have numpy 1.23.5 which is incompatible.\n",
            "dask-expr 1.1.21 requires pandas>=2, but you have pandas 1.5.3 which is incompatible.\n",
            "bigframes 1.42.0 requires numpy>=1.24.0, but you have numpy 1.23.5 which is incompatible.\n",
            "plotnine 0.14.5 requires pandas>=2.2.0, but you have pandas 1.5.3 which is incompatible.\n",
            "jax 0.5.2 requires numpy>=1.25, but you have numpy 1.23.5 which is incompatible.\n",
            "imbalanced-learn 0.13.0 requires numpy<3,>=1.24.3, but you have numpy 1.23.5 which is incompatible.\n",
            "mizani 0.13.1 requires pandas>=2.2.0, but you have pandas 1.5.3 which is incompatible.\n",
            "xarray 2025.1.2 requires numpy>=1.24, but you have numpy 1.23.5 which is incompatible.\n",
            "xarray 2025.1.2 requires pandas>=2.1, but you have pandas 1.5.3 which is incompatible.\n",
            "albumentations 2.0.5 requires numpy>=1.24.4, but you have numpy 1.23.5 which is incompatible.\n",
            "scikit-image 0.25.2 requires networkx>=3.0, but you have networkx 2.8.8 which is incompatible.\n",
            "scikit-image 0.25.2 requires numpy>=1.24, but you have numpy 1.23.5 which is incompatible.\n",
            "nx-cugraph-cu12 25.2.0 requires networkx>=3.2, but you have networkx 2.8.8 which is incompatible.\n",
            "jaxlib 0.5.1 requires numpy>=1.25, but you have numpy 1.23.5 which is incompatible.\n",
            "chex 0.1.89 requires numpy>=1.24.1, but you have numpy 1.23.5 which is incompatible.\n",
            "albucore 0.0.23 requires numpy>=1.24.4, but you have numpy 1.23.5 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed numpy-1.23.5\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "numpy"
                ]
              },
              "id": "4b7a54e7a69e4760a97ce709f05fe346"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from fastapi import FastAPI, Request, HTTPException, Query,Form\n",
        "from fastapi.responses import StreamingResponse\n",
        "from TTS.api import TTS\n",
        "import uvicorn\n",
        "import os\n",
        "import uuid\n",
        "from pyngrok import ngrok\n",
        "import nest_asyncio\n",
        "\n",
        "nest_asyncio.apply()\n",
        "\n",
        "app = FastAPI()\n",
        "OUTPUT_DIR = \"outputs\"\n",
        "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
        "OUTPUT_PATH = os.path.join(OUTPUT_DIR, \"output.mpeg\")\n",
        "\n",
        "device = \"cuda\"\n",
        "tts = TTS(model_name=\"tts_models/en/ljspeech/glow-tts\").to(device)\n",
        "\n",
        "def audio_streamer(file_path: str, chunk_size: int = 1024):\n",
        "    with open(file_path, \"rb\") as audio_file:\n",
        "        while chunk := audio_file.read(chunk_size):\n",
        "            yield chunk\n",
        "\n",
        "@app.post(\"/speak\")\n",
        "async def speak(\n",
        "    request: Request,\n",
        "    model: str = Query(\"tts_models/en/ljspeech/glow-tts\", description=\"text to speech\"),\n",
        "    encoding: str = Query(\"linear16\", description=\"Audio encoding format\"),\n",
        "    container: str = Query(\"wav\", description=\"Output container format\"),\n",
        "    sample_rate: int = Query(24000, description=\"Output audio sample rate in Hz\")\n",
        "):\n",
        "    try:\n",
        "        body = await request.json()\n",
        "    except Exception:\n",
        "        raise HTTPException(status_code=400, detail=\"Invalid JSON payload\")\n",
        "\n",
        "    if \"text\" not in body:\n",
        "        raise HTTPException(status_code=400, detail=\"Missing 'text' in request body\")\n",
        "\n",
        "    text = body[\"text\"]\n",
        "    tts.tts_to_file(text=text, file_path=OUTPUT_PATH)\n",
        "    response_headers = {\n",
        "        \"content-type\": \"audio/mpeg\",\n",
        "        \"kipps-model-name\": model,\n",
        "        \"kipps-model-uuid\": str(uuid.uuid4()),\n",
        "        \"kipps-char-count\": str(len(text)),\n",
        "        \"kipps-request-id\": str(uuid.uuid4())\n",
        "    }\n",
        "    return StreamingResponse(audio_streamer(OUTPUT_PATH), media_type=\"audio/mpeg\", headers=response_headers)\n",
        "\n",
        "ngrok.set_auth_token(\"2v8WxF5ZeHTZz8xCPZoDdKkyNbx_G3XWmHLmGSdb7FiYhYWx\")\n",
        "public_url = ngrok.connect(5000)\n",
        "print(f\"Public URL: {public_url}\")\n",
        "uvicorn.run(app, host=\"0.0.0.0\", port=5000)\n"
      ],
      "metadata": {
        "id": "d-MM6LBCOYlE",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "32d520f9-7fe1-4c38-c27e-a91a44e60be8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "ERROR:asyncio:Task exception was never retrieved\n",
            "future: <Task finished name='Task-7' coro=<Server.serve() done, defined at /usr/local/lib/python3.11/dist-packages/uvicorn/server.py:68> exception=KeyboardInterrupt()>\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/uvicorn/main.py\", line 579, in run\n",
            "    server.run()\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/uvicorn/server.py\", line 66, in run\n",
            "    return asyncio.run(self.serve(sockets=sockets))\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/nest_asyncio.py\", line 30, in run\n",
            "    return loop.run_until_complete(task)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/nest_asyncio.py\", line 92, in run_until_complete\n",
            "    self._run_once()\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/nest_asyncio.py\", line 133, in _run_once\n",
            "    handle._run()\n",
            "  File \"/usr/lib/python3.11/asyncio/events.py\", line 84, in _run\n",
            "    self._context.run(self._callback, *self._args)\n",
            "  File \"/usr/lib/python3.11/asyncio/tasks.py\", line 360, in __wakeup\n",
            "    self.__step()\n",
            "  File \"/usr/lib/python3.11/asyncio/tasks.py\", line 277, in __step\n",
            "    result = coro.send(None)\n",
            "             ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/uvicorn/server.py\", line 69, in serve\n",
            "    with self.capture_signals():\n",
            "  File \"/usr/lib/python3.11/contextlib.py\", line 144, in __exit__\n",
            "    next(self.gen)\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/uvicorn/server.py\", line 330, in capture_signals\n",
            "    signal.raise_signal(captured_signal)\n",
            "KeyboardInterrupt\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " > tts_models/en/ljspeech/glow-tts is already downloaded.\n",
            " > vocoder_models/en/ljspeech/multiband-melgan is already downloaded.\n",
            " > Using model: glow_tts\n",
            " > Setting up Audio Processor...\n",
            " | > sample_rate:22050\n",
            " | > resample:False\n",
            " | > num_mels:80\n",
            " | > log_func:np.log10\n",
            " | > min_level_db:-100\n",
            " | > frame_shift_ms:None\n",
            " | > frame_length_ms:None\n",
            " | > ref_level_db:0\n",
            " | > fft_size:1024\n",
            " | > power:1.1\n",
            " | > preemphasis:0.0\n",
            " | > griffin_lim_iters:60\n",
            " | > signal_norm:False\n",
            " | > symmetric_norm:True\n",
            " | > mel_fmin:50.0\n",
            " | > mel_fmax:7600.0\n",
            " | > pitch_fmin:1.0\n",
            " | > pitch_fmax:640.0\n",
            " | > spec_gain:1.0\n",
            " | > stft_pad_mode:reflect\n",
            " | > max_norm:1.0\n",
            " | > clip_norm:True\n",
            " | > do_trim_silence:True\n",
            " | > trim_db:60\n",
            " | > do_sound_norm:False\n",
            " | > do_amp_to_db_linear:True\n",
            " | > do_amp_to_db_mel:True\n",
            " | > do_rms_norm:False\n",
            " | > db_level:None\n",
            " | > stats_path:None\n",
            " | > base:10\n",
            " | > hop_length:256\n",
            " | > win_length:1024\n",
            " > Vocoder Model: multiband_melgan\n",
            " > Setting up Audio Processor...\n",
            " | > sample_rate:22050\n",
            " | > resample:False\n",
            " | > num_mels:80\n",
            " | > log_func:np.log10\n",
            " | > min_level_db:-100\n",
            " | > frame_shift_ms:None\n",
            " | > frame_length_ms:None\n",
            " | > ref_level_db:0\n",
            " | > fft_size:1024\n",
            " | > power:1.5\n",
            " | > preemphasis:0.0\n",
            " | > griffin_lim_iters:60\n",
            " | > signal_norm:True\n",
            " | > symmetric_norm:True\n",
            " | > mel_fmin:50.0\n",
            " | > mel_fmax:7600.0\n",
            " | > pitch_fmin:0.0\n",
            " | > pitch_fmax:640.0\n",
            " | > spec_gain:1.0\n",
            " | > stft_pad_mode:reflect\n",
            " | > max_norm:4.0\n",
            " | > clip_norm:True\n",
            " | > do_trim_silence:True\n",
            " | > trim_db:60\n",
            " | > do_sound_norm:False\n",
            " | > do_amp_to_db_linear:True\n",
            " | > do_amp_to_db_mel:True\n",
            " | > do_rms_norm:False\n",
            " | > db_level:None\n",
            " | > stats_path:/root/.local/share/tts/vocoder_models--en--ljspeech--multiband-melgan/scale_stats.npy\n",
            " | > base:10\n",
            " | > hop_length:256\n",
            " | > win_length:1024\n",
            " > Generator Model: multiband_melgan_generator\n",
            " > Discriminator Model: melgan_multiscale_discriminator\n",
            "Public URL: NgrokTunnel: \"https://96f6-34-82-189-53.ngrok-free.app\" -> \"http://localhost:5000\"\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:     Started server process [5287]\n",
            "INFO:     Waiting for application startup.\n",
            "INFO:     Application startup complete.\n",
            "INFO:     Uvicorn running on http://0.0.0.0:5000 (Press CTRL+C to quit)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO:     2401:4900:1c64:a894:b0db:a548:f453:37e7:0 - \"GET /docs HTTP/1.1\" 200 OK\n",
            "INFO:     2401:4900:1c64:a894:b0db:a548:f453:37e7:0 - \"GET /openapi.json HTTP/1.1\" 200 OK\n",
            " > Text splitted to sentences.\n",
            "['hello my name is harshita']\n",
            " > Processing time: 2.0672378540039062\n",
            " > Real-time factor: 0.8683060553334756\n",
            "INFO:     54.86.50.139:0 - \"POST /speak HTTP/1.1\" 200 OK\n",
            " > Text splitted to sentences.\n",
            "['hello my name is harshita, you are bad girl']\n",
            " > Processing time: 0.2095654010772705\n",
            " > Real-time factor: 0.05292419247931344\n",
            "INFO:     54.86.50.139:0 - \"POST /speak HTTP/1.1\" 200 OK\n"
          ]
        }
      ]
    }
  ]
}
